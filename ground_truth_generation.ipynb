{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "810f4498",
   "metadata": {},
   "source": [
    "# Legal RAG Evaluation - Ground Truth Dataset Creation\n",
    "\n",
    "This notebook creates a ground truth Q&A dataset for evaluating the legal RAG pipeline.\n",
    "Focus: Data usage questions with large context chunks (2000+ tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "915b4b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import textwrap\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "from legal_rag import LegalRAGBackend\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "149bd215",
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_OPENAI_ENDPOINT = \"https://ironclad-openai-001.openai.azure.com/\"\n",
    "AZURE_OPENAI_API_KEY = \"936856630b764210913d9a8fd6c8212b\"\n",
    "AZURE_DEPLOYMENT_NAME = \"gpt-4o\"\n",
    "\n",
    "rag_backend = LegalRAGBackend()\n",
    "\n",
    "\n",
    "azure_llm = AzureChatOpenAI(\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=\"2023-05-15\",\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    azure_deployment=AZURE_DEPLOYMENT_NAME,\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f06d315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_USAGE_QUERIES = [\n",
    "    \"Can we use client data to develop or test new services?\",\n",
    "    \"client data usage for development and testing\",\n",
    "    \"PHI data usage restrictions\",\n",
    "    \"artificial intelligence machine learning restrictions\",\n",
    "    \"data retention requirements timelines\",\n",
    "    \"client consent requirements data usage\",\n",
    "    \"third-party vendor data processing\",\n",
    "    \"human oversight AI decision making\",\n",
    "    \"IP ownership rights client data\",\n",
    "    \"cloud storage limitations PHI\",\n",
    "    \"data sharing restrictions\",\n",
    "    \"client data anonymization requirements\"\n",
    "]\n",
    "\n",
    "KEY_CLIENTS = rag_backend._load_clients_data()['Client']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56ebc930",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 1500  # Minimum chunk size for large context\n",
    "\n",
    "def get_large_context_chunks(queries: List[str], clients: List[str], top_k: int = 10) -> List[Dict[str, Any]]:\n",
    "    contexts = []\n",
    "    \n",
    "    for query in queries:\n",
    "        for client in clients:\n",
    "            try:\n",
    "                response = rag_backend.query_s3_vector_store(\n",
    "                    query_text=query,\n",
    "                    client_account_filter=client,\n",
    "                    top_k=top_k\n",
    "                )\n",
    "                \n",
    "                if response and 'vectors' in response:\n",
    "                    for vector in response['vectors']:\n",
    "                        metadata = vector.get('metadata', {})\n",
    "                        text = metadata.get('text', '')\n",
    "                        \n",
    "                        if len(text) >= CHUNK_SIZE:\n",
    "                            contexts.append({\n",
    "                                'text': text,\n",
    "                                'client': client,\n",
    "                                'source': metadata.get('s3_path', 'unknown'),\n",
    "                                'document_type': metadata.get('document_type', 'unknown'),\n",
    "                                'query_used': query\n",
    "                            })\n",
    "                            \n",
    "                time.sleep(0.5)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error querying {query} for {client}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "485000bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape_curly_braces(text: str) -> str:\n",
    "    text = str(text)\n",
    "    return text.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "\n",
    "\n",
    "def generate_qa_pair(context: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    system_prompt = textwrap.dedent(f\"\"\"\n",
    "        You are an expert creating multiple-choice questions from legal documents.\n",
    "        Create questions about data usage, privacy, AI/ML restrictions, and related legal topics.\n",
    "        \n",
    "        Your output should be like the following Python dictionary structure:\n",
    "        \n",
    "        {escape_curly_braces({\n",
    "            \"question\": \"Put here the question text\",\n",
    "            \"options\": {\n",
    "                \"A\": \"Option A text\",\n",
    "                \"B\": \"Option B text\",\n",
    "                \"C\": \"Option C text\",\n",
    "                \"D\": \"Option D text\"\n",
    "            },\n",
    "            \"correct_answer\": \"Give the correct option letter (A, B, C, or D)\",\n",
    "            \"explanation\": \"Give a brief explanation of why this is the correct answer\"\n",
    "        })}\n",
    "        \n",
    "        For example:\n",
    "        \n",
    "        {escape_curly_braces({\n",
    "            \"question\": \"What are the data retention requirements for client PHI?\",\n",
    "            \"options\": {\n",
    "                \"A\": \"1 year after contract termination\",\n",
    "                \"B\": \"3 years after contract termination\",\n",
    "                \"C\": \"5 years after contract termination\",\n",
    "                \"D\": \"Permanent retention required\"\n",
    "            },\n",
    "            \"correct_answer\": \"B\",\n",
    "            \"explanation\": \"The contract specifies a 3-year retention period for client PHI after contract termination.\"\n",
    "        })}\n",
    "        \n",
    "        ONLY output the requested dictionary.\n",
    "        \"\"\")\n",
    "    \n",
    "    human_prompt = textwrap.dedent(f\"\"\"\n",
    "        Create a multiple-choice question based on the following legal document text:\n",
    "        \n",
    "        Client: {escape_curly_braces(context['client'])}\n",
    "        Document Type: {escape_curly_braces(context['document_type'])}\n",
    "        \n",
    "        Text:\n",
    "        {escape_curly_braces(context['text'][:3000])}\n",
    "        \"\"\")\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", human_prompt),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Create LangChain chain\n",
    "        chain = prompt | azure_llm\n",
    "        output = chain.invoke({})\n",
    "        \n",
    "        qa_text = output.content.strip()\n",
    "        qa_dict = eval(qa_text)\n",
    "        \n",
    "        qa_dict['source'] = context['source']\n",
    "        qa_dict['client'] = context['client']\n",
    "        qa_dict['document_type'] = context['document_type']\n",
    "        qa_dict['context'] = context['text'][:3000]  # Limit context to first 3000 characters\n",
    "        \n",
    "        return qa_dict\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating Q&A: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5abe9be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_options(qa_list: List[Dict], seed: int = 42) -> List[Dict]:\n",
    "    random.seed(seed)\n",
    "    \n",
    "    for qa in qa_list:\n",
    "        options = list(qa[\"options\"].items())\n",
    "        random.shuffle(options)\n",
    "        \n",
    "        new_option_keys = ['A', 'B', 'C', 'D']\n",
    "        new_options = {new_key: value for new_key, (_, value) in zip(new_option_keys, options)}\n",
    "        \n",
    "        correct_answer_value = qa[\"options\"][qa[\"correct_answer\"]]\n",
    "        correct_answer_key = next(new_key for new_key, value in new_options.items() if value == correct_answer_value)\n",
    "        \n",
    "        qa[\"options\"] = new_options\n",
    "        qa[\"correct_answer\"] = correct_answer_key\n",
    "    \n",
    "    return qa_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d767f7",
   "metadata": {},
   "source": [
    "## Step 1: Collect Large Context Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5555dcb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting large context chunks for data usage topics...\n",
      "Collected 955 context chunks\n",
      "Average length: 1846 characters\n",
      "\n",
      "Context 1:\n",
      "Client: 1199 SEIU National Benefit Funds\n",
      "Doc Type: Work Order / Change Request\n",
      "Length: 1610 chars\n",
      "Preview: This WO #1 may be executed in two or more counterparts, each of which shall be \n",
      "deemed to be an original as against any Party whose signature appears thereon, but all of which \n",
      "together shall constitu...\n"
     ]
    }
   ],
   "source": [
    "print(\"Collecting large context chunks for data usage topics...\")\n",
    "contexts = get_large_context_chunks(DATA_USAGE_QUERIES, KEY_CLIENTS, top_k=5)\n",
    "\n",
    "print(f\"Collected {len(contexts)} context chunks\")\n",
    "print(f\"Average length: {sum(len(c['text']) for c in contexts) / len(contexts):.0f} characters\")\n",
    "\n",
    "for i, ctx in enumerate(contexts[:1]):\n",
    "    print(f\"\\nContext {i+1}:\")\n",
    "    print(f\"Client: {ctx['client']}\")\n",
    "    print(f\"Doc Type: {ctx['document_type']}\")\n",
    "    print(f\"Length: {len(ctx['text'])} chars\")\n",
    "    print(f\"Preview: {ctx['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76c08330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This WO #1 may be executed in two or more counterparts, each of which shall be \n",
      "deemed to be an original as against any Party whose signature appears thereon, but all of which \n",
      "together shall constitute one and the same instrument. IN WITNESS WHEREOF, the undersigned, intending to be legally bound, have executed this \n",
      "WO #1 as of the date of last signature below. Cotiviti, Inc.   \n",
      "1199SEIU National Benefit Fund for Health \n",
      "and Human Service Employees \n",
      "1199SEIU Greater New York Benefit Fund \n",
      "1199SEIU National Benefit Fund for Home \n",
      "Care Employees \n",
      " \n",
      "By:       By:  \n",
      "Name:       Name:  \n",
      "Title:       Title:  \n",
      "Date:       Date:  \n",
      "     \n",
      "       \n",
      "       \n",
      "       \n",
      "       \n",
      "       \n",
      " \n",
      " \n",
      " \n",
      "Angela Scott\n",
      "Chief of Benefit Operations\n",
      "12 / 11 / 2023\n",
      "DocuSign Envelope ID: 4776A02C-397E-465F-82F7-C0CBB6A8A78F\n",
      "Cheri Moehring\n",
      "SVP Clinical Chart Validation\n",
      "December 11, 2023 | 5:25 PM EST\n",
      "EXHIBIT A  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "High Level IT Ro les and \n",
      "Responsib ilities \n",
      "with Deliverables  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Prepared  for:  1199SEIU NATIONAL BENEFIT FUND  \n",
      "FOR HEALTH AND HUMAN SERVICE EMPLOYEES , \n",
      "1199SEIU GREATER NEW YORK BENEFIT FUND , \n",
      "1199SEIU NATIONAL BENEFIT FUND FOR HOME CARE  \n",
      "EMPLOYEES  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "August 22, 2023\n",
      "DocuSign Envelope ID: 4776A02C-397E-465F-82F7-C0CBB6A8A78F\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Scope  \n",
      " \n",
      "Cotiviti  will assign  employees  and/or  subcontractors  to design,  build,  test, and \n",
      "operationalization  of the data interface  between  Client  and Cotiviti. Such  data interface  \n",
      "processes  shall include:  \n",
      "a. Cotiviti  receives  and processes  both the Client’s  prepay,  post adjudication  and paid \n",
      "claim  data;   \n",
      "b.\n"
     ]
    }
   ],
   "source": [
    "print(ctx['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81541094",
   "metadata": {},
   "source": [
    "## Step 2: Generate Q&A Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c5c2c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 200 Q&A pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Q&A:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q&A 1:\n",
      "Q: Under what conditions can either party assign their rights and obligations under this agreement?\n",
      "A: C - Without consent if assigning to a successor of substantially all business assets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Q&A:  10%|█         | 20/200 [00:41<06:19,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q&A 21:\n",
      "Q: What is the duration of the confidentiality obligations after the expiration or termination of the agreement?\n",
      "A: C - 3 years after the Effective Date\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Q&A:  20%|██        | 40/200 [01:26<05:56,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q&A 41:\n",
      "Q: What action must a user take if hardware and software requirements change for electronic disclosures?\n",
      "A: C - Receive an email notification and have the right to withdraw consent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Q&A:  30%|███       | 60/200 [02:08<04:43,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q&A 61:\n",
      "Q: What is the definition of \"Medically Necessary\" under the provisions of this SOW?\n",
      "A: B - Services or supplies determined to be appropriate and necessary for the symptoms, diagnosis, or treatment of the medical condition.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Q&A:  40%|████      | 80/200 [02:51<04:15,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q&A 81:\n",
      "Q: What is the consequence of withdrawing consent to receive notices and disclosures electronically?\n",
      "A: C - Slower transaction speed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Q&A:  50%|█████     | 100/200 [03:34<03:32,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q&A 101:\n",
      "Q: What restrictions are imposed on the Client regarding the Verisk IP?\n",
      "A: C - Client cannot distribute or sublicense Verisk IP.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Q&A:  60%|██████    | 120/200 [04:14<02:41,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q&A 121:\n",
      "Q: What is excluded from Comparative Data according to the General VH Master Services Agreement?\n",
      "A: B - Protected Health Information\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Q&A:  70%|███████   | 140/200 [04:55<02:10,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q&A 141:\n",
      "Q: What is required for remote computing sessions that involve access to Customer Confidential Information according to the document?\n",
      "A: B - Access via a secure Virtual Private Network (VPN)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Q&A:  80%|████████  | 160/200 [05:36<01:21,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q&A 161:\n",
      "Q: What is the data sharing requirement for Licensee under the Agreement?\n",
      "A: B - Licensee must provide claims and enrollment data semi-annually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Q&A:  90%|█████████ | 180/200 [06:18<00:42,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q&A 181:\n",
      "Q: What remedy may be sought if a party breaches the agreement according to the legal document text?\n",
      "A: B - Specific performance and/or injunctive relief\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Q&A: 100%|█████████▉| 199/200 [06:59<00:02,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated 200 Q&A pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TARGET_QA_COUNT = 200\n",
    "qa_dataset = []\n",
    "\n",
    "random.seed(42)\n",
    "selected_contexts = random.sample(contexts, min(TARGET_QA_COUNT, len(contexts)))\n",
    "\n",
    "print(f\"Generating {TARGET_QA_COUNT} Q&A pairs...\")\n",
    "\n",
    "for i, context in enumerate(tqdm(selected_contexts, desc=\"Generating Q&A\")):\n",
    "    qa_pair = generate_qa_pair(context)\n",
    "    \n",
    "    if qa_pair:\n",
    "        qa_dataset.append(qa_pair)\n",
    "        if i%20==0:\n",
    "            print(f\"\\nQ&A {len(qa_dataset)}:\")\n",
    "            print(f\"Q: {qa_pair['question']}\")\n",
    "            print(f\"A: {qa_pair['correct_answer']} - {qa_pair['options'][qa_pair['correct_answer']]}\")\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    if len(qa_dataset) >= TARGET_QA_COUNT:\n",
    "        break\n",
    "\n",
    "print(f\"\\nGenerated {len(qa_dataset)} Q&A pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d55a313",
   "metadata": {},
   "source": [
    "## Step 3: Process and Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86f1bc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ids_to_qa(qa_list: List[Dict]) -> List[Dict]:\n",
    "    for i, qa in enumerate(qa_list):\n",
    "        qa['id'] = f\"qa_{i+1}\"\n",
    "    return qa_list\n",
    "\n",
    "\n",
    "def evaluate_qa_relevance(qa_item: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Evaluate the relevance of the context to the client and query before factuality assessment.\"\"\"\n",
    "    \n",
    "    system_prompt = textwrap.dedent(f\"\"\"\n",
    "        You are an expert at evaluating document quality and relevance.\n",
    "        Your task is to determine if the provided context contains meaningful legal information relevant to the client and query.\n",
    "        \n",
    "        Evaluation criteria:\n",
    "        1. Context should contain substantive legal information (contracts, policies, agreements)\n",
    "        2. Content should be relevant to the client mentioned\n",
    "        3. Context should not be junk data (headers, footers, metadata, gibberish)\n",
    "        4. Information should be useful for generating meaningful legal questions\n",
    "        \n",
    "        Your output should be like the following Python dictionary structure:\n",
    "        \n",
    "        {escape_curly_braces({\n",
    "            \"relevance\": 1, \n",
    "            \"critic_explanation\": \"Explanation of why the context is relevant or not relevant\"\n",
    "        })}\n",
    "        \n",
    "        ONLY output the requested dictionary.\n",
    "        \"\"\")\n",
    "    \n",
    "    human_prompt = textwrap.dedent(f\"\"\"\n",
    "        Evaluate the relevance of this context for generating legal Q&A:\n",
    "        \n",
    "        **Client:** {escape_curly_braces(qa_item.get('client', 'unknown'))}\n",
    "        **Document Type:** {escape_curly_braces(qa_item.get('document_type', 'unknown'))}\n",
    "        **Query Used:** {escape_curly_braces(qa_item.get('query_used', 'unknown'))}\n",
    "        \n",
    "        **Context:**\n",
    "        {escape_curly_braces(qa_item.get('context', ''))}\n",
    "        \"\"\")\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", human_prompt),\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        chain = prompt | azure_llm\n",
    "        output = chain.invoke({})\n",
    "        evaluation_text = output.content.strip()\n",
    "        \n",
    "        import re\n",
    "        evaluation_text = re.sub(r'\\\\u[0-9a-fA-F]{4}', '', evaluation_text)\n",
    "        evaluation_text = re.sub(r'```[a-zA-Z]*\\n?', '', evaluation_text)\n",
    "        evaluation_text = re.sub(r'\\s+', ' ', evaluation_text).strip()\n",
    "        \n",
    "        dict_match = re.search(r'\\{.*\\}', evaluation_text, re.DOTALL)\n",
    "        if dict_match:\n",
    "            evaluation_text = dict_match.group(0)\n",
    "        \n",
    "        evaluation_dict = eval(evaluation_text)\n",
    "        \n",
    "        if not isinstance(evaluation_dict, dict) or 'relevance' not in evaluation_dict or 'critic_explanation' not in evaluation_dict:\n",
    "            raise ValueError(\"Invalid response format from LLM\")\n",
    "        \n",
    "        qa_item_evaluated = qa_item.copy()\n",
    "        qa_item_evaluated['relevance'] = evaluation_dict['relevance']\n",
    "        qa_item_evaluated['relevance_explanation'] = evaluation_dict['critic_explanation']\n",
    "        \n",
    "        return qa_item_evaluated\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating Q&A relevance: {e}\")\n",
    "        qa_item_error = qa_item.copy()\n",
    "        qa_item_error['relevance'] = 0\n",
    "        qa_item_error['relevance_explanation'] = f\"Error during evaluation: {e}\"\n",
    "        return qa_item_error\n",
    "\n",
    "\n",
    "def evaluate_qa_factuality(qa_item: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Evaluate the factuality of a Q&A pair given its context using an LLM critic.\"\"\"\n",
    "    \n",
    "    system_prompt = textwrap.dedent(f\"\"\"\n",
    "        You are an expert legal document analyst and fact-checker.\n",
    "        Your task is to evaluate whether a multiple-choice question and its correct answer are factually accurate based on the provided legal document context.\n",
    "        \n",
    "        Evaluation criteria:\n",
    "        1. The question should be answerable from the given context\n",
    "        2. The correct answer should be directly supported by the context\n",
    "        3. The answer should not contain information not present in the context\n",
    "        4. The question should not be ambiguous or misleading\n",
    "        \n",
    "        Your output should be like the following Python dictionary structure:\n",
    "        \n",
    "        {escape_curly_braces({\n",
    "            \"factuality\": 1, \n",
    "            \"critic_explanation\": \"Explanation of why the Q&A is factually correct or incorrect based on the context\"\n",
    "        })}\n",
    "        \n",
    "        ONLY output the requested dictionary.\n",
    "        \"\"\")\n",
    "    \n",
    "    human_prompt = textwrap.dedent(f\"\"\"\n",
    "        Evaluate the factuality of this Q&A pair based on the provided legal document context:\n",
    "        \n",
    "        **Context:**\n",
    "        {escape_curly_braces(qa_item.get('context', ''))}\n",
    "        \n",
    "        **Question:**\n",
    "        {escape_curly_braces(qa_item['question'])}\n",
    "        \n",
    "        **Options:**\n",
    "        {escape_curly_braces(qa_item['options'])}\n",
    "        \n",
    "        **Marked Correct Answer:**\n",
    "        {escape_curly_braces(qa_item['correct_answer'])} - {escape_curly_braces(qa_item['options'][qa_item['correct_answer']])}\n",
    "        \n",
    "        **Explanation:**\n",
    "        {escape_curly_braces(qa_item['explanation'])}\n",
    "        \n",
    "        **Client:** {escape_curly_braces(qa_item.get('client', 'unknown'))}\n",
    "        **Document Type:** {escape_curly_braces(qa_item.get('document_type', 'unknown'))}\n",
    "        \"\"\")\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", human_prompt),\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        chain = prompt | azure_llm\n",
    "        output = chain.invoke({})\n",
    "        evaluation_text = output.content.strip()\n",
    "        \n",
    "        import re\n",
    "        evaluation_text = re.sub(r'\\\\u[0-9a-fA-F]{4}', '', evaluation_text)\n",
    "        evaluation_text = re.sub(r'```[a-zA-Z]*\\n?', '', evaluation_text)\n",
    "        evaluation_text = re.sub(r'\\s+', ' ', evaluation_text).strip()\n",
    "        \n",
    "        dict_match = re.search(r'\\{.*\\}', evaluation_text, re.DOTALL)\n",
    "        if dict_match:\n",
    "            evaluation_text = dict_match.group(0)\n",
    "        \n",
    "        evaluation_dict = eval(evaluation_text)\n",
    "        \n",
    "        if not isinstance(evaluation_dict, dict) or 'factuality' not in evaluation_dict or 'critic_explanation' not in evaluation_dict:\n",
    "            raise ValueError(\"Invalid response format from LLM\")\n",
    "        \n",
    "        qa_item_evaluated = qa_item.copy()\n",
    "        qa_item_evaluated['factuality'] = evaluation_dict['factuality']\n",
    "        qa_item_evaluated['critic_explanation'] = evaluation_dict['critic_explanation']\n",
    "        \n",
    "        return qa_item_evaluated\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating Q&A factuality: {e}\")\n",
    "        print(f\"Raw LLM response: {output.content[:200] if 'output' in locals() else 'No output captured'}...\")\n",
    "        qa_item_error = qa_item.copy()\n",
    "        qa_item_error['factuality'] = 0\n",
    "        qa_item_error['critic_explanation'] = f\"Error during evaluation: {e}\"\n",
    "        return qa_item_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2967b875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 200 Q&A pairs to qa_list.json\n",
      "\n",
      "Dataset summary:\n",
      "Clients: {'Ochsner Health System': 8, 'ATRIO Health Plans': 8, 'Accenture': 7, 'Capital Health Plan, Inc.': 9, 'Crumdale Partners - Better Benefits': 12, 'Maryland Physicians Care (MPC)': 13, 'Blue Shield of California': 5, \"Martin's Point Health Care\": 9, 'ACA Health Benefits Fund': 7, 'Wellth, Inc.': 7, 'Advise Insurance': 5, '1199 SEIU National Benefit Funds': 2, 'Arthur J Gallagher & Co.': 1, '7-Eleven': 12, 'Abarca Health LLC': 10, 'Community First Health Plans': 5, 'Albertsons Companies, Inc.': 9, 'Blue Cross and Blue Shield of Nebraska': 9, 'Boston Benefit Partners': 3, 'AdvantMed': 9, 'Sierra Health Plan of Nevada': 6, 'Self Insured Services Co': 11, 'The Standard': 7, 'Humana': 4, 'ACT.md': 8, 'Inogen': 3, 'Horizon Blue Cross Blue Shield of New Jersey': 2, 'Aetna Life Insurance Company': 4, 'AIA Health Insurance Pty Ltd': 4, 'AxialHealthcare': 1}\n",
      "Document types: {'unknown': 124, 'NDA': 45, 'MSA': 22, 'Work Order / Change Request': 2, 'DUA': 1, 'BAA': 1, 'SOW': 1, 'Amendment': 1, 'Addendum': 3}\n",
      "\n",
      "Sample Q&A:\n",
      "\n",
      "1. Under what conditions can either party assign their rights and obligations under this agreement?\n",
      "  ✓ A: Without consent if assigning to a successor of substantially all business assets.\n",
      "    B: Without consent if the agreement is executed in counterparts.\n",
      "    C: With consent only if the assignment is for less than half of the obligations.\n",
      "    D: With prior written consent from the other party.\n",
      "\n",
      "2. What is the primary restriction on the use and disclosure of Confidential Information under the NDA between ATRIO Health Plans and the Receiving Party?\n",
      "    A: Confidential Information must be shared with all employees.\n",
      "  ✓ B: Confidential Information can only be used in relation to the Potential Transaction.\n",
      "    C: Confidential Information can be used for any business purpose.\n",
      "    D: Confidential Information can be disclosed to third parties without permission.\n"
     ]
    }
   ],
   "source": [
    "qa_dataset_shuffled = shuffle_options(qa_dataset, seed=42)\n",
    "qa_dataset_shuffled = add_ids_to_qa(qa_dataset_shuffled)\n",
    "\n",
    "with open(\"qa_list.json\", \"w\") as f:\n",
    "    json.dump(qa_dataset_shuffled, f, indent=4)\n",
    "\n",
    "print(f\"Saved {len(qa_dataset_shuffled)} Q&A pairs to qa_list.json\")\n",
    "\n",
    "print(\"\\nDataset summary:\")\n",
    "clients = [qa['client'] for qa in qa_dataset_shuffled]\n",
    "doc_types = [qa['document_type'] for qa in qa_dataset_shuffled]\n",
    "\n",
    "from collections import Counter\n",
    "print(f\"Clients: {dict(Counter(clients))}\")\n",
    "print(f\"Document types: {dict(Counter(doc_types))}\")\n",
    "\n",
    "print(\"\\nSample Q&A:\")\n",
    "for i, qa in enumerate(qa_dataset_shuffled[:2]):\n",
    "    print(f\"\\n{i+1}. {qa['question']}\")\n",
    "    for opt, text in qa['options'].items():\n",
    "        marker = \"✓\" if opt == qa['correct_answer'] else \" \"\n",
    "        print(f\"  {marker} {opt}: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd45896",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate Q&A Factuality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03da848a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance and factuality of generated Q&A pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q&A 1 Evaluation:\n",
      "Question: Under what conditions can either party assign their rights and obligations under this agreement?...\n",
      "Relevance: 1\n",
      "Factuality: 1\n",
      "Critic: The marked correct answer is factually accurate based on the provided context. The context explicitly states that either party can assign their rights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A:  10%|█         | 20/200 [00:54<08:39,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q&A 21 Evaluation:\n",
      "Question: What is the duration of the confidentiality obligations after the expiration or termination of the a...\n",
      "Relevance: 1\n",
      "Factuality: 1\n",
      "Critic: The question is answerable from the given context and the correct answer is supported by the context. The document states that the obligations regardi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A:  20%|██        | 40/200 [01:51<08:06,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q&A 41 Evaluation:\n",
      "Question: What action must a user take if hardware and software requirements change for electronic disclosures...\n",
      "Relevance: 0\n",
      "Factuality: 0\n",
      "Critic: Skipped factuality check due to low relevance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A:  30%|███       | 60/200 [02:46<06:21,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q&A 61 Evaluation:\n",
      "Question: What is the definition of \"Medically Necessary\" under the provisions of this SOW?...\n",
      "Relevance: 1\n",
      "Factuality: 1\n",
      "Critic: The correct answer A is factually accurate as it directly aligns with the definition of \"Medically Necessary\" provided in the context. The context spe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A:  40%|████      | 80/200 [03:43<05:24,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q&A 81 Evaluation:\n",
      "Question: What is the consequence of withdrawing consent to receive notices and disclosures electronically?...\n",
      "Relevance: 1\n",
      "Factuality: 1\n",
      "Critic: The question is directly answerable from the context provided, and the marked correct answer (A: Slower transaction speed) is explicitly supported by ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A:  50%|█████     | 100/200 [04:40<04:38,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q&A 101 Evaluation:\n",
      "Question: What restrictions are imposed on the Client regarding the Verisk IP?...\n",
      "Relevance: 1\n",
      "Factuality: 1\n",
      "Critic: The question is answerable from the given context, and the correct answer is directly supported by the context. The document clearly states that the C...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A:  60%|██████    | 120/200 [05:36<03:27,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q&A 121 Evaluation:\n",
      "Question: What is excluded from Comparative Data according to the General VH Master Services Agreement?...\n",
      "Relevance: 1\n",
      "Factuality: 1\n",
      "Critic: The question is directly answerable from the provided context, and the marked correct answer \"D - Protected Health Information\" is supported by the st...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A:  70%|███████   | 140/200 [06:33<02:42,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q&A 141 Evaluation:\n",
      "Question: What is required for remote computing sessions that involve access to Customer Confidential Informat...\n",
      "Relevance: 1\n",
      "Factuality: 1\n",
      "Critic: The question is directly answerable from the provided context, which specifies that permitted and authorized remote computing sessions involving acces...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A:  76%|███████▌  | 151/200 [07:07<02:25,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating Q&A relevance: unterminated string literal (detected at line 1) (<string>, line 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A:  80%|████████  | 160/200 [07:35<01:58,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q&A 161 Evaluation:\n",
      "Question: What is the data sharing requirement for Licensee under the Agreement?...\n",
      "Relevance: 1\n",
      "Factuality: 1\n",
      "Critic: The question is answerable from the given context and the marked correct answer is directly supported by the context. The agreement indeed requires th...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A:  90%|█████████ | 180/200 [08:29<00:57,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q&A 181 Evaluation:\n",
      "Question: What remedy may be sought if a party breaches the agreement according to the legal document text?...\n",
      "Relevance: 1\n",
      "Factuality: 1\n",
      "Critic: The question is answerable from the context and the correct answer, C, is directly supported by the context. The document specifies that specific perf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A: 100%|██████████| 200/200 [09:29<00:00,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed evaluation for 200 Q&A pairs\n",
      "Relevant: 181/200 (90.5%)\n",
      "Factually correct: 178/200 (89.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating relevance and factuality of generated Q&A pairs...\")\n",
    "evaluated_qa_dataset = []\n",
    "\n",
    "for i, qa_item in enumerate(tqdm(qa_dataset_shuffled, desc=\"Evaluating Q&A\")):\n",
    "    # First evaluate relevance\n",
    "    relevance_evaluated = evaluate_qa_relevance(qa_item)\n",
    "    \n",
    "    # Only evaluate factuality if relevant\n",
    "    if relevance_evaluated['relevance'] == 1:\n",
    "        final_qa = evaluate_qa_factuality(relevance_evaluated)\n",
    "    else:\n",
    "        final_qa = relevance_evaluated\n",
    "        final_qa['factuality'] = 0\n",
    "        final_qa['critic_explanation'] = \"Skipped factuality check due to low relevance\"\n",
    "    \n",
    "    evaluated_qa_dataset.append(final_qa)\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        print(f\"\\nQ&A {i+1} Evaluation:\")\n",
    "        print(f\"Question: {qa_item['question'][:100]}...\")\n",
    "        print(f\"Relevance: {final_qa['relevance']}\")\n",
    "        print(f\"Factuality: {final_qa['factuality']}\")\n",
    "        print(f\"Critic: {final_qa.get('critic_explanation', final_qa.get('relevance_explanation', ''))[:150]}...\")\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"\\nCompleted evaluation for {len(evaluated_qa_dataset)} Q&A pairs\")\n",
    "\n",
    "relevant_count = sum(1 for qa in evaluated_qa_dataset if qa['relevance'] == 1)\n",
    "factual_count = sum(1 for qa in evaluated_qa_dataset if qa['factuality'] == 1)\n",
    "print(f\"Relevant: {relevant_count}/{len(evaluated_qa_dataset)} ({relevant_count/len(evaluated_qa_dataset)*100:.1f}%)\")\n",
    "print(f\"Factually correct: {factual_count}/{len(evaluated_qa_dataset)} ({factual_count/len(evaluated_qa_dataset)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95bc29a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered datasets:\n",
      "- Relevant: 181 Q&A pairs\n",
      "- Factually correct: 178 Q&A pairs\n",
      "- Both relevant and factual: 178 Q&A pairs\n",
      "\n",
      "Saved datasets:\n",
      "- qa_list_evaluated.json: 200 Q&A pairs (with all scores)\n",
      "- qa_list_relevant_only.json: 181 relevant Q&A pairs\n",
      "- qa_list_factual_only.json: 178 factually correct Q&A pairs\n",
      "- qa_list_relevant_and_factual.json: 178 high-quality Q&A pairs\n"
     ]
    }
   ],
   "source": [
    "# filter datasets by relevance and factuality\n",
    "relevant_qa_dataset = [qa for qa in evaluated_qa_dataset if qa['relevance'] == 1]\n",
    "factual_qa_dataset = [qa for qa in evaluated_qa_dataset if qa['factuality'] == 1]\n",
    "relevant_and_factual_qa_dataset = [qa for qa in evaluated_qa_dataset if qa['relevance'] == 1 and qa['factuality'] == 1]\n",
    "\n",
    "print(f\"Filtered datasets:\")\n",
    "print(f\"- Relevant: {len(relevant_qa_dataset)} Q&A pairs\")\n",
    "print(f\"- Factually correct: {len(factual_qa_dataset)} Q&A pairs\") \n",
    "print(f\"- Both relevant and factual: {len(relevant_and_factual_qa_dataset)} Q&A pairs\")\n",
    "\n",
    "# save all datasets\n",
    "with open(\"qa_list_evaluated.json\", \"w\") as f:\n",
    "    json.dump(evaluated_qa_dataset, f, indent=4)\n",
    "\n",
    "with open(\"qa_list_relevant_only.json\", \"w\") as f:\n",
    "    json.dump(relevant_qa_dataset, f, indent=4)\n",
    "\n",
    "with open(\"qa_list_factual_only.json\", \"w\") as f:\n",
    "    json.dump(factual_qa_dataset, f, indent=4)\n",
    "\n",
    "with open(\"qa_list_relevant_and_factual.json\", \"w\") as f:\n",
    "    json.dump(relevant_and_factual_qa_dataset, f, indent=4)\n",
    "\n",
    "print(f\"\\nSaved datasets:\")\n",
    "print(f\"- qa_list_evaluated.json: {len(evaluated_qa_dataset)} Q&A pairs (with all scores)\")\n",
    "print(f\"- qa_list_relevant_only.json: {len(relevant_qa_dataset)} relevant Q&A pairs\")\n",
    "print(f\"- qa_list_factual_only.json: {len(factual_qa_dataset)} factually correct Q&A pairs\")\n",
    "print(f\"- qa_list_relevant_and_factual.json: {len(relevant_and_factual_qa_dataset)} high-quality Q&A pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546c5a09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evals_legal_rag (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
